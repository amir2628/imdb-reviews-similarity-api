# –°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è DistilBERT –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ IMDB

import os
import time
import torch
from datasets import load_dataset
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments

def check_model_exists():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø–æ–ª–Ω–æ–π –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    model_path = "./fine_tuned_model"
    
    # –°–ø–∏—Å–æ–∫ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏
    required_files = [
        "config.json",           # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        "pytorch_model.bin",     # –í–µ—Å–∞ –º–æ–¥–µ–ª–∏
        "special_tokens_map.json",        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        "tokenizer_config.json", # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        "vocab.txt"              # –°–ª–æ–≤–∞—Ä—å
    ]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    if not os.path.exists(model_path):
        print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–∏ {model_path} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤
    missing_files = []
    for file_name in required_files:
        file_path = os.path.join(model_path, file_name)
        if not os.path.exists(file_path):
            missing_files.append(file_name)
        elif os.path.getsize(file_path) == 0:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π
            missing_files.append(f"{file_name} (–ø—É—Å—Ç–æ–π)")
    
    if missing_files:
        print(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∏–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏: {missing_files}")
        return False
    
    # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
    try:
        print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏...")
        model = DistilBertForSequenceClassification.from_pretrained(
            model_path, 
            local_files_only=True
        )
        tokenizer = DistilBertTokenizer.from_pretrained(
            model_path, 
            local_files_only=True
        )
        print("‚úì –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –º–æ–¥–µ–ª—å –ø—Ä–æ—à–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏")
        return True
        
    except Exception as e:
        print(f"–ú–æ–¥–µ–ª—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∞ –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω–∞—è: {e}")
        return False

def safe_clean_incomplete_model():
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –Ω–µ–ø–æ–ª–Ω–æ–π –∏–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    model_path = "./fine_tuned_model"
    if os.path.exists(model_path):
        print("–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏...")
        try:
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã –ø–æ –æ–¥–Ω–æ–º—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å volume
            for root, dirs, files in os.walk(model_path, topdown=False):
                for file in files:
                    try:
                        os.remove(os.path.join(root, file))
                    except OSError as e:
                        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {file}: {e}")
                for dir in dirs:
                    try:
                        os.rmdir(os.path.join(root, dir))
                    except OSError as e:
                        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é {dir}: {e}")
            
            # –ü–æ–ø—ã—Ç–∫–∞ —É–¥–∞–ª–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            try:
                os.rmdir(model_path)
                print("‚úì –ù–µ–ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞")
            except OSError as e:
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {e}")
                print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π...")
                
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ: {e}")
            print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")

def create_training_marker():
    """–°–æ–∑–¥–∞–µ—Ç –º–∞—Ä–∫–µ—Ä –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è"""
    marker_path = "./training_in_progress.marker"
    with open(marker_path, 'w') as f:
        f.write(f"Training started at {time.time()}")
    print("‚úì –ú–∞—Ä–∫–µ—Ä –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–Ω")

def remove_training_marker():
    """–£–¥–∞–ª—è–µ—Ç –º–∞—Ä–∫–µ—Ä –æ–±—É—á–µ–Ω–∏—è"""
    marker_path = "./training_in_progress.marker"
    if os.path.exists(marker_path):
        os.remove(marker_path)
        print("‚úì –ú–∞—Ä–∫–µ—Ä –æ–±—É—á–µ–Ω–∏—è —É–¥–∞–ª–µ–Ω")

def main():
    print("–ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –æ–∂–∏–¥–∞–Ω–∏—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    time.sleep(10)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏
    if check_model_exists():
        print("–ü–æ–ª–Ω–∞—è –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ...")
        return
    else:
        print("–ú–æ–¥–µ–ª—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω–∞—è, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ...")
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ä–∫–µ—Ä –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è
        create_training_marker()
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –æ—á–∏—â–∞–µ–º –Ω–µ–ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
        safe_clean_incomplete_model()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ IMDB
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ IMDB...")
    try:
        print("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Hugging Face –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ IMDB...")
        dataset = load_dataset("imdb")
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        train_dataset = dataset["train"].select(range(1000))  # 1000 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        eval_dataset = dataset["test"].select(range(200))     # 200 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_dataset)} –æ–±—É—á–∞—é—â–∏—Ö –∏ {len(eval_dataset)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        
    except Exception as e:
        print(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç IMDB: {e}")
        print("–û–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –±–µ–∑ –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ IMDB.")
        remove_training_marker()
        raise e
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    try:
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
        remove_training_marker()
        raise e
    
    # –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=256)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    train_tokenized = train_dataset.map(tokenize_function, batched=True)
    eval_tokenized = eval_dataset.map(tokenize_function, batched=True)
    
    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ label –≤ labels –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Trainer
    train_tokenized = train_tokenized.rename_column("label", "labels")
    eval_tokenized = eval_tokenized.rename_column("label", "labels")
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    eval_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
    
    print("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ DistilBERT...")
    try:
        model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
        print("–ú–æ–¥–µ–ª—å DistilBERT —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        remove_training_marker()
        raise e
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è (—Å–æ–≥–ª–∞—Å–Ω–æ –∑–∞–¥–∞–Ω–∏—é: 1-2 —ç–ø–æ—Ö–∏)
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=2,  # 1-2 —ç–ø–æ—Ö–∏ –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –∑–∞–¥–∞–Ω–∏–∏
        per_device_train_batch_size=8,
        per_device_eval_batch_size=16,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=50,
        save_strategy="no",
        evaluation_strategy="steps",
        eval_steps=200,
        report_to=[],
        push_to_hub=False,
        dataloader_pin_memory=False,
        remove_unused_columns=True,
        load_best_model_at_end=False,
        disable_tqdm=False,
    )
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_tokenized,
        eval_dataset=eval_tokenized,
    )
    
    # –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ IMDB –¥–∞—Ç–∞—Å–µ—Ç–µ
    print("–ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –¥–æ–æ–±—É—á–µ–Ω–∏–µ DistilBERT –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ IMDB...")
    print("–≠—Ç–æ –∑–∞–π–º–µ—Ç 10-20 –º–∏–Ω—É—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã...")
    
    try:
        trainer.train()
        print("–û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        remove_training_marker()
        raise e
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    try:
        os.makedirs("./fine_tuned_model", exist_ok=True)
        model.save_pretrained("./fine_tuned_model")
        tokenizer.save_pretrained("./fine_tuned_model")
        print("–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
        remove_training_marker()
        raise e
    
    # –£–¥–∞–ª—è–µ–º –º–∞—Ä–∫–µ—Ä –æ–±—É—á–µ–Ω–∏—è
    remove_training_marker()
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    if check_model_exists():
        print("–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print("–ú–æ–¥–µ–ª—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ./fine_tuned_model")
        print("–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    else:
        print("–û–®–ò–ë–ö–ê: –ú–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
        raise Exception("–ú–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

if __name__ == "__main__":
    main()